# -*- coding: utf-8 -*-
"""NeuroWatch_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1caFnlQKuv6fzFRSFHXyQew4L793yW85S
"""

print("NeuroWatch AI Environment Ready")

!pip install mediapipe==0.10.13 opencv-python librosa soundfile gradio numpy pandas

import cv2
import mediapipe as mp
import numpy as np
import pandas as pd
import librosa
import soundfile as sf
import gradio as gr
from datetime import datetime

def analyze_gait(video_path):
    cap = cv2.VideoCapture(video_path)
    frames = 0
    movement_scores = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = pose.process(rgb)

        if result.pose_landmarks:
            hip = result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP]
            knee = result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE]
            movement_scores.append(abs(hip.y - knee.y))

        frames += 1

    cap.release()

    if len(movement_scores) == 0:
        return 0

    score = round(np.mean(movement_scores) * 100, 2)
    return score

print("Gait Test Score:", analyze_gait("videos/sample.mp4"))

def analyze_voice(audio_path):
    y, sr = librosa.load(audio_path)
    pitch = np.mean(librosa.yin(y, fmin=50, fmax=300))
    stability = max(0, 100 - abs(pitch - 150))
    return round(stability, 2)

import numpy as np

def extract_gait_features(landmarks):
    """
    landmarks: list of mediapipe pose landmarks
    returns: stride_variability, movement_energy
    """

    left_hip = landmarks[23]
    right_hip = landmarks[24]
    left_knee = landmarks[25]
    right_knee = landmarks[26]

    hip_dist = abs(left_hip.y - right_hip.y)
    knee_dist = abs(left_knee.y - right_knee.y)

    stride_variability = hip_dist
    movement_energy = knee_dist

    return stride_variability, movement_energy

dummy_landmarks = pose.process(
    np.zeros((480,640,3),dtype=np.uint8)
)

print("Function Loaded")

def calculate_gait_score(stride_var, movement_energy):
    """
    Lower variability = better gait
    Higher movement energy = better mobility
    """

    # Normalize rough ranges
    stride_score = max(0, 100 - (stride_var * 400))
    energy_score = min(100, movement_energy * 400)

    final_score = (stride_score + energy_score) / 2

    return round(final_score, 2)

print(calculate_gait_score(0.05, 0.06))

import cv2
import numpy as np
import mediapipe as mp

mp_pose = mp.solutions.pose
pose = mp_pose.Pose()

def analyze_gait_video(video_path):
    cap = cv2.VideoCapture(video_path)

    knee_y = []
    hip_y = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = pose.process(rgb)

        if results.pose_landmarks:
            landmarks = results.pose_landmarks.landmark

            knee = landmarks[mp_pose.PoseLandmark.LEFT_KNEE].y
            hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP].y

            knee_y.append(knee)
            hip_y.append(hip)

    cap.release()

    if len(knee_y) < 10:
        return "Not enough motion detected"

    stride_variability = np.std(knee_y)
    movement_energy = np.std(hip_y)

    gait_score = calculate_gait_score(stride_variability, movement_energy)

    return f"Gait Score: {gait_score}"

analyze_gait_video("/content/walk.mp4")

import gradio as gr

def gait_ui(video):
    result = analyze_gait_video(video)
    return result

gait_app = gr.Interface(
    fn=gait_ui,
    inputs=gr.Video(label="Upload Walking Video"),
    outputs=gr.Textbox(label="Gait Analysis Result"),
    title="NeuroWatch - Gait Analysis",
    description="Upload a short walking video to analyze gait stability."
)

gait_app.launch()

def voice_ui(audio):
    score = analyze_voice(audio)
    return f"Voice Stability Score: {score}"

voice_app = gr.Interface(
    fn=voice_ui,
    inputs=gr.Audio(sources=["microphone"], type="filepath", label="Record or Upload Voice"),
    outputs=gr.Textbox(label="Voice Analysis Result"),
    title="NeuroWatch - Voice Stability",
    description="Record a short voice sample to analyze speech stability."
)

voice_app.launch()

import mediapipe as mp

mp_pose = mp.solutions.pose
pose = mp_pose.Pose()

print("MediaPipe Pose Initialized Successfully")

import gradio as gr
import random # Import random for the sync_smartwatch function
from datetime import datetime

with gr.Blocks() as app:
    HISTORY = [] # Initialize HISTORY list here

    gr.Markdown("# ðŸ§  NeuroWatch â€“ AI Neurodegenerative Monitoring Platform")

    with gr.Tabs():

        with gr.Tab("Dashboard"):
            dashboard_output = gr.Textbox(label="Health Overview")

        with gr.Tab("Gait Analysis"):
            gait_video = gr.Video(label="Upload Walking Video")
            gait_btn = gr.Button("Analyze Gait")
            gait_result = gr.Textbox(label="Gait Result")

        with gr.Tab("Voice Analysis"):
            voice_audio = gr.Audio(sources=["microphone"], type="filepath")
            voice_btn = gr.Button("Analyze Voice")
            voice_result = gr.Textbox(label="Voice Result")

        with gr.Tab("Smartwatch"):
            watch_btn = gr.Button("Sync Device")
            watch_output = gr.Textbox(label="Smartwatch Data")

        with gr.Tab("Lifestyle"):
            breakfast = gr.Textbox(label="Breakfast")
            lunch = gr.Textbox(label="Lunch")
            dinner = gr.Textbox(label="Dinner")
            sleep = gr.Slider(0,12,label="Sleep Hours")
            lifestyle_btn = gr.Button("Save Lifestyle")
            lifestyle_out = gr.Textbox()

        with gr.Tab("History"):
            history_btn = gr.Button("View History")
            history_out = gr.Textbox(lines=10)

    # -------- CONNECT GAIT BUTTON --------
    def gait_wrapper(video):
        return analyze_gait_video(video)

    gait_btn.click(
        fn=gait_wrapper,
        inputs=gait_video,
        outputs=gait_result
    )

    # -------- CONNECT VOICE BUTTON --------
    def voice_wrapper(audio):
        return f"Voice Stability Score: {analyze_voice(audio)}"

    voice_btn.click(
        fn=voice_wrapper,
        inputs=voice_audio,
        outputs=voice_result
    )

    # -------- CONNECT SMARTWATCH BUTTON --------
    def sync_smartwatch():
        data = {
            "Heart Rate": random.randint(60, 95),
            "Blood Pressure": f"{random.randint(110,130)}/{random.randint(70,85)}",
            "Tremor Level": random.randint(5, 40),
            "Muscle Movement": random.randint(60, 95)
        }
        return f"""
Heart Rate: {data['Heart Rate']} bpm
Blood Pressure: {data['Blood Pressure']}
Tremor Level: {data['Tremor Level']}
Muscle Movement: {data['Muscle Movement']}
"""

    watch_btn.click(
        fn=sync_smartwatch,
        inputs=None,
        outputs=watch_output
    )

    # -------- HISTORY FUNCTIONS --------
    def save_history(gait, voice, watch, breakfast_val=None, lunch_val=None, dinner_val=None, sleep_val=None):
        HISTORY.append({
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "gait": gait,
            "voice": voice,
            "watch": watch,
            "breakfast": breakfast_val,
            "lunch": lunch_val,
            "dinner": dinner_val,
            "sleep": sleep_val
        })
        return f"Saved data at {HISTORY[-1]['timestamp']}"

    def view_history():
        if not HISTORY:
            return "No history recorded yet."
        formatted_history = []
        for entry in HISTORY:
            lifestyle_str = ""
            if entry.get("breakfast") or entry.get("lunch") or entry.get("dinner") or (entry.get("sleep") is not None):
                lifestyle_str += f"  Breakfast: {entry.get('breakfast', 'N/A')}\n"
                lifestyle_str += f"  Lunch: {entry.get('lunch', 'N/A')}\n"
                lifestyle_str += f"  Dinner: {entry.get('dinner', 'N/A')}\n"
                lifestyle_str += f"  Sleep: {entry.get('sleep', 'N/A')} hours\n"

            formatted_history.append(
                f"Timestamp: {entry['timestamp']}\n"
                f"  Gait: {entry['gait']}\n"
                f"  Voice: {entry['voice']}\n"
                f"  Watch: {entry['watch']}\n"
                f"{lifestyle_str}"
                f"--------------------"
            )
        return "\n".join(formatted_history)

    # -------- CONNECT DASHBOARD REFRESH BUTTON --------
    def refresh_dashboard_ui_wrapper():
        try:
            gait = float(gait_result.value.split(":")[-1]) if gait_result.value and "Gait Score:" in gait_result.value else 0.0
            voice = float(voice_result.value.split(":")[-1]) if voice_result.value and "Voice Stability Score:" in voice_result.value else 0.0
            tremor_str = watch_output.value.split("Tremor Level:")[1].split("\n")[0].strip() if watch_output.value and "Tremor Level:" in watch_output.value else "0"
            tremor = int(tremor_str)
        except (ValueError, IndexError):
            # Handle cases where data is not yet available or in incorrect format
            gait = 0.0 # Default value if parsing fails
            voice = 0.0
            tremor = 0
            # Return a message to the dashboard if data is not ready
            return "Not enough data yet for dashboard. Please analyze gait/voice and sync smartwatch."

        # Update dashboard display
        dashboard_text = update_dashboard(gait_result.value, voice_result.value, watch_output.value)

        # Save current state to history (with None for lifestyle data)
        save_history(gait, voice, watch_output.value, None, None, None, None)

        return dashboard_text

    gr.Button("Refresh Dashboard").click(
        refresh_dashboard_ui_wrapper,
        outputs=dashboard_output
    )

    # Connect History Button
    history_btn.click(
        fn=view_history,
        inputs=None,
        outputs=history_out
    )

    # -------- CONNECT LIFESTYLE BUTTON --------
    lifestyle_btn.click(
        fn=lambda b,l,d,s: save_history(
            gait_result.value,
            voice_result.value,
            watch_output.value,
            b, l, d, s
        ),
        inputs=[breakfast,lunch,dinner,sleep],
        outputs=lifestyle_out
    )

app.launch()

def compute_risk(gait, voice, tremor):
    score = 0
    if gait < 60: score += 2
    if voice < 60: score += 2
    if tremor > 30: score += 2

    if score <= 2:
        return "Low Risk"
    elif score <= 4:
        return "Moderate Risk"
    else:
        return "High Risk"

def update_dashboard(gait_text, voice_text, watch_text):
    try:
        gait = float(gait_text.split(":")[-1])
        voice = float(voice_text.split(":")[-1])
        tremor = int(watch_text.split("Tremor Level:")[1].split("\n")[0])
    except:
        return "Not enough data yet"

    risk = compute_risk(gait, voice, tremor)

    return f"""
Gait Score: {gait}
Voice Stability: {voice}
Tremor Level: {tremor}
--------------------
Neuro Risk Level: {risk}
"""